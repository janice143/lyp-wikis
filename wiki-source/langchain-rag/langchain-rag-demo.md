# **ç¬¬ä¸ƒç« ï¼šLangChain + RAG æ¡ˆä¾‹å®æˆ˜**

æœ¬ç« å°†é€šè¿‡**äº”ä¸ª RAG å®æˆ˜æ¡ˆä¾‹**ï¼Œå±•ç¤ºå¦‚ä½•åˆ©ç”¨ **LangChain + å‘é‡æ•°æ®åº“ + å¤§è¯­è¨€æ¨¡å‹** æ­å»ºé«˜æ•ˆçš„ **çŸ¥è¯†é—®ç­”ã€æ–‡æ¡£æ£€ç´¢ã€ç¦»çº¿ RAG ç³»ç»Ÿã€æµ·é‡æ•°æ®æ£€ç´¢åŠäº¤äº’å¼ RAG åŠ©æ‰‹**ã€‚

---

## **æ¡ˆä¾‹ 1ï¼šåŸºäº OpenAI API + FAISS æ­å»º RAG çŸ¥è¯†é—®ç­”**

**åœºæ™¯**ï¼šæ„å»ºä¸€ä¸ª RAG çŸ¥è¯†é—®ç­”ç³»ç»Ÿï¼Œç”¨æˆ·å¯ä»¥åŸºäºè‡ªå®šä¹‰æ–‡æ¡£è¿›è¡Œé—®ç­”ï¼Œåç«¯ä½¿ç”¨ **FAISS å‘é‡æ•°æ®åº“** è¿›è¡Œæ£€ç´¢ï¼Œå‰ç«¯è°ƒç”¨ **OpenAI API** ç”Ÿæˆå›ç­”ã€‚

### **æ­¥éª¤**

1. **å®‰è£…ä¾èµ–**

```bash
pip install langchain openai faiss-cpu tiktoken
```

2. **å­˜å‚¨æ–‡æ¡£åˆ° FAISS**

```python
from langchain.vectorstores import FAISS
from langchain.embeddings.openai import OpenAIEmbeddings

texts = ["RAG æ˜¯ä¸€ç§ç»“åˆæ£€ç´¢å’Œç”Ÿæˆçš„ AI æŠ€æœ¯ã€‚", "LangChain æ˜¯ç”¨äºæ„å»º RAG ç³»ç»Ÿçš„æ¡†æ¶ã€‚"]
embeddings = OpenAIEmbeddings()
vectorstore = FAISS.from_texts(texts, embedding=embeddings)
```

3. **æ„å»ºé—®ç­”ç³»ç»Ÿ**

```python
from langchain.chains import RetrievalQA
from langchain.chat_models import ChatOpenAI

llm = ChatOpenAI(model_name="gpt-4")
retriever = vectorstore.as_retriever()
qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)

query = "ä»€ä¹ˆæ˜¯ RAGï¼Ÿ"
response = qa_chain.run(query)
print(response)
```

âœ… **ç‰¹ç‚¹**ï¼š

- é€‚ç”¨äºå°å‹çŸ¥è¯†åº“é—®ç­”ï¼Œé€Ÿåº¦å¿«ã€å­˜å‚¨è½»é‡ã€‚
- å¯ç”¨äºä¼ä¸šå†…éƒ¨ FAQã€äº§å“æ–‡æ¡£æŸ¥è¯¢ç­‰ã€‚

---

## **æ¡ˆä¾‹ 2ï¼šä½¿ç”¨ ChromaDB æ„å»ºæœ¬åœ°æ–‡æ¡£æ£€ç´¢ç³»ç»Ÿ**

**åœºæ™¯**ï¼šChromaDB æ˜¯ä¸€ä¸ªè½»é‡çº§çš„æœ¬åœ°å‘é‡æ•°æ®åº“ï¼Œé€‚ç”¨äº**ä¸ªäººçŸ¥è¯†åº“ã€ä»£ç æ–‡æ¡£æŸ¥è¯¢ã€ç§‘ç ”æ–‡çŒ®ç®¡ç†**ã€‚

### **æ­¥éª¤**

1. **å®‰è£… ChromaDB**

```bash
pip install chromadb langchain
```

2. **åˆ›å»ºæ•°æ®åº“å¹¶å­˜å‚¨æ–‡æ¡£**

```python
import chromadb
from langchain.vectorstores import Chroma
from langchain.embeddings.openai import OpenAIEmbeddings

chroma_client = chromadb.PersistentClient(path="chroma_db")
vectorstore = Chroma(persist_directory="chroma_db", embedding_function=OpenAIEmbeddings())

texts = ["LangChain è®© AI åº”ç”¨å¼€å‘æ›´ç®€å•ã€‚", "ChromaDB æ˜¯ä¸€ä¸ªæœ¬åœ°å‘é‡æ•°æ®åº“ã€‚"]
vectorstore.add_texts(texts)
```

3. **æŸ¥è¯¢çŸ¥è¯†åº“**

```python
retriever = vectorstore.as_retriever()
query = "LangChain æœ‰ä»€ä¹ˆä¼˜åŠ¿ï¼Ÿ"
docs = retriever.get_relevant_documents(query)
print(docs)
```

âœ… **ç‰¹ç‚¹**ï¼š

- é€‚ç”¨äºæœ¬åœ°å­˜å‚¨ï¼Œæ”¯æŒç¦»çº¿æŸ¥è¯¢ã€‚
- é€‚åˆ**ä¸ªäººçŸ¥è¯†åº“ã€ç§‘ç ”è®ºæ–‡æ£€ç´¢**ã€‚

---

## **æ¡ˆä¾‹ 3ï¼šLangChain + Hugging Face æ­å»ºç¦»çº¿ RAG ç³»ç»Ÿ**

**åœºæ™¯**ï¼šå¦‚æœä½ ä¸æƒ³ä¾èµ– OpenAIï¼Œå¯ä»¥ä½¿ç”¨ Hugging Face çš„**å¼€æºåµŒå…¥æ¨¡å‹**ï¼Œæ­å»º**å®Œå…¨ç¦»çº¿çš„ RAG ç³»ç»Ÿ**ã€‚

### **æ­¥éª¤**

1. **å®‰è£… Hugging Face ä¾èµ–**

```bash
pip install langchain transformers sentence-transformers
```

2. **åŠ è½½æœ¬åœ°åµŒå…¥æ¨¡å‹**

```python
from langchain.embeddings import HuggingFaceEmbeddings

embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
```

3. **å­˜å‚¨å‘é‡æ•°æ®**

```python
from langchain.vectorstores import FAISS

texts = ["LangChain æ˜¯ä¸€ä¸ªç”¨äºæ„å»º LLM åº”ç”¨çš„æ¡†æ¶ã€‚", "Hugging Face æä¾›äº†å¾ˆå¤šå¼€æºæ¨¡å‹ã€‚"]
vectorstore = FAISS.from_texts(texts, embedding=embeddings)
```

4. **è¿›è¡ŒæŸ¥è¯¢**

```python
retriever = vectorstore.as_retriever()
query = "LangChain æ˜¯ä»€ä¹ˆï¼Ÿ"
docs = retriever.get_relevant_documents(query)
print(docs)
```

âœ… **ç‰¹ç‚¹**ï¼š

- **ä¸ä¾èµ– API**ï¼Œå¯åœ¨æœ¬åœ°æ‰§è¡Œï¼Œé€‚ç”¨äº**é«˜éšç§åœºæ™¯**ã€‚
- é€‚ç”¨äº**ä¼ä¸šå†…ç½‘ã€æ³•å¾‹æ³•è§„ã€æ•æ„Ÿæ•°æ®æŸ¥è¯¢**ã€‚

---

## **æ¡ˆä¾‹ 4ï¼šä½¿ç”¨ Pinecone è¿›è¡Œå¤§è§„æ¨¡æ•°æ®æ£€ç´¢**

**åœºæ™¯**ï¼šå¦‚æœæ•°æ®é‡è¾ƒå¤§ï¼ˆå¦‚**ä¼ä¸šæ–‡æ¡£ç®¡ç†ã€æµ·é‡è®ºæ–‡ã€é‡‘èæ•°æ®æ£€ç´¢**ï¼‰ï¼Œå¯ä»¥ä½¿ç”¨ **Pinecone** è¿›è¡Œé«˜æ•ˆæŸ¥è¯¢ã€‚

### **æ­¥éª¤**

1. **å®‰è£… Pinecone**

```bash
pip install pinecone-client langchain
```

2. **åˆå§‹åŒ– Pinecone**

```python
import pinecone
pinecone.init(api_key="YOUR_API_KEY", environment="us-west1-gcp")

from langchain.vectorstores import Pinecone
vectorstore = Pinecone(index_name="my-index")
```

3. **å­˜å‚¨æ•°æ®**

```python
texts = ["é‡‘èå¸‚åœºè¶‹åŠ¿é¢„æµ‹ã€‚", "æœºå™¨å­¦ä¹ åœ¨é‡‘èé¢†åŸŸçš„åº”ç”¨ã€‚"]
vectorstore.add_texts(texts)
```

4. **æŸ¥è¯¢æ•°æ®**

```python
retriever = vectorstore.as_retriever()
query = "é‡‘èå¸‚åœºé¢„æµ‹"
docs = retriever.get_relevant_documents(query)
print(docs)
```

âœ… **ç‰¹ç‚¹**ï¼š

- é€‚ç”¨äº**è¶…å¤§è§„æ¨¡æ•°æ®å­˜å‚¨**ï¼Œæ”¯æŒç™¾ä¸‡çº§å‘é‡æœç´¢ã€‚
- é€‚åˆ **ä¼ä¸šæ–‡æ¡£åº“ã€é‡‘èåˆ†æã€ä¸“åˆ©æœç´¢**ã€‚

---

## **æ¡ˆä¾‹ 5ï¼šStreamlit æ‰“é€ äº¤äº’å¼ RAG çŸ¥è¯†åŠ©æ‰‹**

**åœºæ™¯**ï¼šæ„å»ºä¸€ä¸ª **äº¤äº’å¼ Web UI**ï¼Œè®©ç”¨æˆ·è¾“å…¥é—®é¢˜ï¼Œç³»ç»Ÿä» RAG çŸ¥è¯†åº“ä¸­æ£€ç´¢å¹¶è¿”å›ç­”æ¡ˆã€‚

### **æ­¥éª¤**

1. **å®‰è£… Streamlit**

```bash
pip install streamlit
```

2. **åˆ›å»º `app.py`**

```python
import streamlit as st
from langchain.chains import RetrievalQA
from langchain.chat_models import ChatOpenAI

st.title("RAG çŸ¥è¯†é—®ç­”åŠ©æ‰‹")

query = st.text_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜:")

if st.button("æœç´¢"):
    retriever = vectorstore.as_retriever()
    qa_chain = RetrievalQA.from_chain_type(llm=ChatOpenAI(), retriever=retriever)
    response = qa_chain.run(query)
    st.write(response)
```

3. **è¿è¡Œ Streamlit**

```bash
streamlit run app.py
```

ç„¶åè®¿é—® `http://localhost:8501`ï¼Œå³å¯ä½¿ç”¨ **RAG çŸ¥è¯†åŠ©æ‰‹** è¿›è¡Œé—®ç­”ã€‚

âœ… **ç‰¹ç‚¹**ï¼š

- **å¯è§†åŒ–äº¤äº’**ï¼Œé€‚åˆ**ä¼ä¸šå†…éƒ¨é—®ç­”ã€äº§å“æ”¯æŒã€æ³•å¾‹å’¨è¯¢**ã€‚
- **åŸºäº LangChain + Streamlit**ï¼Œæ˜“äºæ‰©å±•ã€‚

---

## **æ€»ç»“**

æœ¬ç« é€šè¿‡ **äº”ä¸ª RAG æ¡ˆä¾‹**ï¼Œå±•ç¤ºäº†ä¸åŒåœºæ™¯ä¸‹çš„ RAG åº”ç”¨ï¼š
1ï¸âƒ£ **OpenAI + FAISS**ï¼šé€‚åˆ**å°å‹çŸ¥è¯†åº“é—®ç­”**ã€‚  
2ï¸âƒ£ **ChromaDB æœ¬åœ°å­˜å‚¨**ï¼šé€‚åˆ**ä¸ªäººçŸ¥è¯†åº“ã€ç§‘ç ”æ–‡çŒ®ç®¡ç†**ã€‚  
3ï¸âƒ£ **Hugging Face ç¦»çº¿ RAG**ï¼šé€‚åˆ**æ—  APIã€æ•°æ®éšç§é«˜çš„åœºæ™¯**ã€‚  
4ï¸âƒ£ **Pinecone å¤§è§„æ¨¡æ£€ç´¢**ï¼šé€‚ç”¨äº**é‡‘èã€æ³•å¾‹ã€ä¼ä¸šæ–‡æ¡£å­˜å‚¨**ã€‚  
5ï¸âƒ£ **Streamlit å¯è§†åŒ– RAG**ï¼šæ‰“é€ **äº¤äº’å¼ AI åŠ©æ‰‹**ã€‚

åœ¨ä¸‹ä¸€ç« èŠ‚ï¼Œæˆ‘ä»¬å°†è®¨è®º **RAG çš„æŒ‘æˆ˜ä¸æœªæ¥å‘å±•**ï¼ŒåŒ…æ‹¬ **æ•°æ®æ›´æ–°ã€å¹»è§‰é—®é¢˜ã€è®¡ç®—æˆæœ¬ä¼˜åŒ–ç­‰**ï¼ ğŸš€
